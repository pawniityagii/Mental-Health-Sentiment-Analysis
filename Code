# Install required libraries (only once per runtime)
!pip install nltk scikit-learn seaborn

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the missing punkt_tab model

# Sample dataset
data = {
    'text': [
        "I feel hopeless and tired of everything.",
        "Today was a good day! I'm feeling optimistic.",
        "Anxious about the upcoming exams.",
        "I had a panic attack this morning.",
        "Feeling relaxed and peaceful after meditation.",
        "Everything feels so overwhelming lately.",
        "I'm really excited about the weekend plans.",
        "Struggling to get out of bed most mornings."
        "I feel like a loser because I do not have a group of friends.",
        "I have friends but I do not have a group of them to be with while I’m at college.",
        "I feel like a loser because I’m always alone.",
        "Unfortunately I’m to blame because I am extremely introverted and self conscious.",
        "I’m self conscious to the point where I won’t be friends with people because acquaintances mocked them frequently.",
        "I’m very lonely.",
        "I began to use weed as a coping mechanism for my depression and we all know how that has turned out.",
        "I’m very low risk because I believe that I have more to live for.",
        "I’m just tired of feeling alone."
    ]
}
df = pd.DataFrame(data)

# Preprocess text
def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    clean_tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]
    return ' '.join(clean_tokens)

df['clean_text'] = df['text'].apply(preprocess_text)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['clean_text'])

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(X)
cluster_labels = {0: "Anxious", 1: "Optimistic", 2: "Depressed"}
df["Sentiment Label"] = df["cluster"].map(cluster_labels)

# PCA for visualization
pca = PCA(n_components=2)
reduced = pca.fit_transform(X.toarray())

# Visualize clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=df['Sentiment Label'], palette='Set2')
plt.title('Mental Health Sentiment Clusters')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.grid(True)
plt.show()

# Show the result
df[['text', 'cluster', 'Sentiment Label']]
